
.. blogpost::
    :title: Ajout de références modules
    :keywords: nmt
    :date: 2018-04-05
    :categories: biblio

    Dans :ref:`l-nmt-traduction-auto` :

    * `Neural Machine Translation <https://github.com/lvapeab/nmt-keras/blob/master/examples/documentation/neural_machine_translation.pdf>`_
    * `marian-nmt <https://marian-nmt.github.io/>`_
    * `NMT-Keras <http://nmt-keras.readthedocs.io/en/latest/resources.html>`_

    Dans :ref:`l-ml2a-object-detection` :

    * `Pascal VOC Dataset <https://github.com/Microsoft/CNTK/tree/master/Examples/Image/DataSets/Pascal>`_

    Dans :ref:`l-ml2a-reconstruction-image` :

    * `Deep Image Prior <https://arxiv.org/pdf/1711.10925.pdf>`_

    Dans :ref:`l-td2a-reinforcement-learning` :

    * `A reinforcement learning algorithm for building collaboration in multi-agent systems <https://arxiv.org/pdf/1711.10574.pdf>`_

    Dans :ref:`l-ml2a-communities` :

    * `Discovering the hidden community structure of public transportation networks <https://arxiv.org/pdf/1801.03857.pdf>`_

    Dans :ref:`l-ml2a-autolearning`:

    * `Generating Neural Networks with Neural Networks <https://arxiv.org/abs/1801.01952>`_

    Dans :ref:`l-ml2a-reddim` :

    * `mQAPViz: A divide-and-conquer multi-objective optimization algorithm to compute large data visualizations <https://arxiv.org/abs/1804.00656>`_

    Dans :ref:`l-td2a-ml-extensions` :

    * `Generalized Random Forests <https://arxiv.org/abs/1610.01271>`_
    * `Gradient Boosting With Piece-Wise Linear Regression Trees <https://arxiv.org/abs/1802.05640>`_,
      ils affirment être meilleur que :epkg:`xgboost` et :epkg:`lightgbm`
      (implémentation `GBDT-PL <https://github.com/GBDT-PL/GBDT-PL>`_)

    Dans :ref:`l-ml2a-graph-embedding` :

    * `Graph2Seq: Graph to Sequence Learning with Attention-based Neural Networks <https://arxiv.org/abs/1804.00823>`_

    Dans :ref:`l-ml2a-seqlearn` :

    * `A ten-minute introduction to sequence-to-sequence learning in Keras <https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html>`_
    * `Translation with a Sequence to Sequence Network and Attention <http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html>`_
    * `Neural Machine Translation in Linear Time <https://arxiv.org/pdf/1610.10099.pdf>`_
    * `Attention Is All You Need <https://arxiv.org/pdf/1706.03762.pdf>`_

    Dans :ref:`l-ml2a-testab` :

    * `Should We Condition on the Test for Pre-trends in Difference-in-Difference Designs? <https://arxiv.org/abs/1804.01208>`_

    Dans :ref:`l-ml2a-deep-gan` :

    * `Synthesizing Programs for Images using Reinforced Adversarial Learning <https://arxiv.org/abs/1804.01118>`_
    * `Flipped-Adversarial AutoEncoders <https://arxiv.org/abs/1802.04504>`_

    Dans :ref:`l-td2a-reinforcement-learning` :

    * `Renewal Monte Carlo: Renewal theory based reinforcement learning <https://arxiv.org/abs/1804.01116>`_
    * `Universal Planning Networks <https://arxiv.org/abs/1804.00645>`_ :
      utilisation de l'apprentissage par renforcement pour caler la progression
      du bras d'un robot vers la saisie d'une pièce lorsque le chemin est obstrué.

    Dans :ref:`l-ml2a-resolu-sketch` :

    * `SketchMate: Deep Hashing for Million-Scale Human Sketch Retrieval <https://arxiv.org/abs/1804.01401>`_

    Dans :ref:`l-ml2a-selvar` :

    * `Variable selection using pseudo-variables <https://arxiv.org/abs/1804.01201>`_ :
      l'article utilise la pénalisation pour classer les variables par importance,
      plus le modèle est pénalisé (type :epkg:`Lasso`), plus il réduit le nombre de variables.

    Dans :ref:`l-ml2aresolu-socnet` :

    * `Real-time Detection of Content Polluters in Partially Observable Twitter Networks <https://arxiv.org/abs/1804.01235>`_

    Dans :ref:`l-interpretabilite-ml` :

    * `Interpretable Policies for Reinforcement Learning by Genetic Programming <https://arxiv.org/abs/1712.04170>`_

    Dans :ref:`l-ml2a-resolu-detpartobj` :

    * `Structured Set Matching Networks for One-Shot Part Labeling <https://arxiv.org/abs/1712.01867>`_

    Dans :ref:`l-ml2a-resolu-detexpr` :

    * `Prediction and Localization of Student Engagement in the Wild <https://arxiv.org/abs/1804.00858>`_

    Dans :ref:`l-ml2a-resolu-detobj` :

    * `Automatic Salient Object Detection for Panoramic Images Using Region Growing and Fixation Prediction Model <https://arxiv.org/abs/1710.04071>`_

    Dans :ref:`l-ml2a-resolu-detobj3d` :

    * `PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space <https://arxiv.org/abs/1706.02413>`_
    * `3DContextNet: K-d Tree Guided Hierarchical Learning of Point Clouds Using Local and Global Contextual Cues <https://arxiv.org/abs/1711.11379>`_

    Inclassables :

    * `Robust Fusion Methods for Structured Big Data <https://arxiv.org/pdf/1804.01858.pdf>`_
    * `A Method for Finding Trends in Software Research <https://arxiv.org/abs/1608.08100>`_
    * `Feedback GAN (FBGAN) for DNA: a Novel Feedback-Loop Architecture for Optimizing Protein Functions <https://arxiv.org/abs/1804.01694>`_
    * `Near-Optimality Recovery of Linear and N-Convex Functions on Unions of Convex Sets <https://arxiv.org/abs/1804.00355>`_
    * `Hidden Talents of the Variational Autoencoder <https://arxiv.org/abs/1706.05148>`_ :
      les auto-encoders sont en quelque sorte des :epkg:`ACP` non linéaire,
      l'article le décrit de façon plus formelle.
    * `Minimax Filter: Learning to Preserve Privacy from Inference Attacks <http://jmlr.org/papers/v18/16-501.html>`_ :
      l'article se pose la question de savoir comment empêcher qu'un modèle complexe
      soit utilisée pour une autre tâche. L'exemple choisi est celui d'un modèle de
      reconnaissance d'émotions dans un visable, comment éviter que celui-ci
      ne soit utilisé à l'identification des visages ?
    * `Knowledge Graph Completion via Complex Tensor Factorization <http://jmlr.org/papers/v18/16-563.html>`_ :
      l'article s'intéresse aux graphes, sociaux, savoir. Comment classer une relation en
      relation hiérarchique ?
    * `ScreenerNet: Learning Self-Paced Curriculum for Deep Neural Networks <https://arxiv.org/abs/1801.00904>`_ :
      l'article essaye d'améliorer l'apprentissage d'un réseau de neurones
      en pondérant les observations de la base d'apprentissage.

    Un module :

    * :epkg:`thorpy` qui implémente des fenêtres façon
      :epkg:`tkinter` pour :epkg:`pygame`

    https://arxiv.org/abs/1711.11379
    https://arxiv.org/abs/1711.00804
    https://arxiv.org/abs/1710.04071
    https://arxiv.org/abs/1804.00858
